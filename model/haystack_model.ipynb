{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP/9LatfOZlNWvsi720QiWr"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1gEb6Tj9ndZ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cebba671-c9eb-4b78-e0a6-6c6c9b6ca699"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/deepset-ai/haystack.git@ef9e4f4467a2e265bad72b048a1a3186e40969b1\n",
            "  Cloning https://github.com/deepset-ai/haystack.git (to revision ef9e4f4467a2e265bad72b048a1a3186e40969b1) to /tmp/pip-req-build-bzhhp0do\n",
            "  Running command git clone -q https://github.com/deepset-ai/haystack.git /tmp/pip-req-build-bzhhp0do\n",
            "  Running command git rev-parse -q --verify 'sha^ef9e4f4467a2e265bad72b048a1a3186e40969b1'\n",
            "  Running command git fetch -q https://github.com/deepset-ai/haystack.git ef9e4f4467a2e265bad72b048a1a3186e40969b1\n",
            "  Running command git checkout -q ef9e4f4467a2e265bad72b048a1a3186e40969b1\n",
            "Collecting farm==0.4.3\n",
            "  Downloading farm-0.4.3.tar.gz (153 kB)\n",
            "\u001b[K     |████████████████████████████████| 153 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting fastapi\n",
            "  Downloading fastapi-0.75.2-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 546 kB/s \n",
            "\u001b[?25hCollecting uvicorn\n",
            "  Downloading uvicorn-0.17.6-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting gunicorn\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from farm-haystack==0.2.1) (1.3.5)\n",
            "Collecting psycopg2-binary\n",
            "  Downloading psycopg2_binary-2.9.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 48.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from farm-haystack==0.2.1) (0.0)\n",
            "Collecting elasticsearch\n",
            "  Downloading elasticsearch-8.1.3-py3-none-any.whl (373 kB)\n",
            "\u001b[K     |████████████████████████████████| 373 kB 42.6 MB/s \n",
            "\u001b[?25hCollecting elastic-apm\n",
            "  Downloading elastic_apm-6.9.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (374 kB)\n",
            "\u001b[K     |████████████████████████████████| 374 kB 37.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: coverage in /usr/local/lib/python3.7/dist-packages (from farm-haystack==0.2.1) (3.7.1)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 45.4 MB/s \n",
            "\u001b[?25hCollecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8 MB 49.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from farm==0.4.3->farm-haystack==0.2.1) (57.4.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from farm==0.4.3->farm-haystack==0.2.1) (0.37.1)\n",
            "Collecting torch==1.4.0\n",
            "  Downloading torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4 MB 6.5 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from farm==0.4.3->farm-haystack==0.2.1) (4.64.0)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.22.3-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 57.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from farm==0.4.3->farm-haystack==0.2.1) (2.23.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from farm==0.4.3->farm-haystack==0.2.1) (1.4.1)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting mlflow==1.0.0\n",
            "  Downloading mlflow-1.0.0-py3-none-any.whl (47.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.7 MB 1.5 MB/s \n",
            "\u001b[?25hCollecting transformers==2.7.0\n",
            "  Downloading transformers-2.7.0-py3-none-any.whl (544 kB)\n",
            "\u001b[K     |████████████████████████████████| 544 kB 69.7 MB/s \n",
            "\u001b[?25hCollecting dotmap==1.3.0\n",
            "  Downloading dotmap-1.3.0-py3-none-any.whl (8.9 kB)\n",
            "Collecting Werkzeug==0.16.1\n",
            "  Downloading Werkzeug-0.16.1-py2.py3-none-any.whl (327 kB)\n",
            "\u001b[K     |████████████████████████████████| 327 kB 57.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from farm==0.4.3->farm-haystack==0.2.1) (1.1.4)\n",
            "Collecting flask-restplus\n",
            "  Downloading flask_restplus-0.13.0-py2.py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 43.4 MB/s \n",
            "\u001b[?25hCollecting flask-cors\n",
            "  Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from farm==0.4.3->farm-haystack==0.2.1) (0.3.4)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.11.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.2 MB 44.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.4.3->farm-haystack==0.2.1) (1.3.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.4.3->farm-haystack==0.2.1) (3.17.3)\n",
            "Collecting simplejson\n",
            "  Downloading simplejson-3.17.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (130 kB)\n",
            "\u001b[K     |████████████████████████████████| 130 kB 67.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.4.3->farm-haystack==0.2.1) (1.15.0)\n",
            "Collecting querystring-parser\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.4.3->farm-haystack==0.2.1) (3.13)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.7.7-py3-none-any.whl (210 kB)\n",
            "\u001b[K     |████████████████████████████████| 210 kB 73.7 MB/s \n",
            "\u001b[?25hCollecting docker>=3.6.0\n",
            "  Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 65.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.4.3->farm-haystack==0.2.1) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.4.3->farm-haystack==0.2.1) (2.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.4.3->farm-haystack==0.2.1) (1.21.6)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.4.3->farm-haystack==0.2.1) (1.4.35)\n",
            "Requirement already satisfied: sqlparse in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.4.3->farm-haystack==0.2.1) (0.4.2)\n",
            "Collecting gitpython>=2.1.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 29.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow==1.0.0->farm==0.4.3->farm-haystack==0.2.1) (0.4)\n",
            "Collecting databricks-cli>=0.8.0\n",
            "  Downloading databricks-cli-0.16.6.tar.gz (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 676 kB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.7.0->farm==0.4.3->farm-haystack==0.2.1) (3.6.0)\n",
            "Collecting tokenizers==0.5.2\n",
            "  Downloading tokenizers-0.5.2-cp37-cp37m-manylinux1_x86_64.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 31.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.7.0->farm==0.4.3->farm-haystack==0.2.1) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 49.7 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 55.4 MB/s \n",
            "\u001b[?25hCollecting pyjwt>=1.7.0\n",
            "  Downloading PyJWT-2.3.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.0->mlflow==1.0.0->farm==0.4.3->farm-haystack==0.2.1) (3.2.0)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.0->mlflow==1.0.0->farm==0.4.3->farm-haystack==0.2.1) (0.8.9)\n",
            "Collecting websocket-client>=0.32.0\n",
            "  Downloading websocket_client-1.3.2-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.3 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from gitpython>=2.1.0->mlflow==1.0.0->farm==0.4.3->farm-haystack==0.2.1) (4.2.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->farm==0.4.3->farm-haystack==0.2.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->farm==0.4.3->farm-haystack==0.2.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->farm==0.4.3->farm-haystack==0.2.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->farm==0.4.3->farm-haystack==0.2.1) (2021.10.8)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from alembic->mlflow==1.0.0->farm==0.4.3->farm-haystack==0.2.1) (4.11.3)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.0-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->mlflow==1.0.0->farm==0.4.3->farm-haystack==0.2.1) (5.7.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->mlflow==1.0.0->farm==0.4.3->farm-haystack==0.2.1) (1.1.2)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 299 kB/s \n",
            "\u001b[?25hCollecting botocore<1.26.0,>=1.25.3\n",
            "  Downloading botocore-1.25.3-py3-none-any.whl (8.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.7 MB 41.4 MB/s \n",
            "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 59.4 MB/s \n",
            "\u001b[?25hCollecting elastic-transport<9,>=8\n",
            "  Downloading elastic_transport-8.1.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 5.7 MB/s \n",
            "\u001b[?25h  Downloading elastic_transport-8.1.1-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.7 MB/s \n",
            "\u001b[?25h  Downloading elastic_transport-8.1.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.8 MB/s \n",
            "\u001b[?25h  Downloading elastic_transport-8.0.1-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 5.1 MB/s \n",
            "\u001b[?25h  Downloading elastic_transport-8.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 4.7 MB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of elasticsearch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting elasticsearch\n",
            "  Downloading elasticsearch-8.1.2-py3-none-any.whl (372 kB)\n",
            "\u001b[K     |████████████████████████████████| 372 kB 67.9 MB/s \n",
            "\u001b[?25h  Downloading elasticsearch-8.1.1-py3-none-any.whl (372 kB)\n",
            "\u001b[K     |████████████████████████████████| 372 kB 70.4 MB/s \n",
            "\u001b[?25h  Downloading elasticsearch-8.1.0-py3-none-any.whl (373 kB)\n",
            "\u001b[K     |████████████████████████████████| 373 kB 46.0 MB/s \n",
            "\u001b[?25h  Downloading elasticsearch-8.0.1-py3-none-any.whl (372 kB)\n",
            "\u001b[K     |████████████████████████████████| 372 kB 58.8 MB/s \n",
            "\u001b[?25h  Downloading elasticsearch-8.0.0-py3-none-any.whl (369 kB)\n",
            "\u001b[K     |████████████████████████████████| 369 kB 58.3 MB/s \n",
            "\u001b[?25h  Downloading elasticsearch-7.17.3-py2.py3-none-any.whl (385 kB)\n",
            "\u001b[K     |████████████████████████████████| 385 kB 57.7 MB/s \n",
            "\u001b[?25hCollecting starlette==0.17.1\n",
            "  Downloading starlette-0.17.1-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2\n",
            "  Downloading pydantic-1.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.9 MB 41.5 MB/s \n",
            "\u001b[?25hCollecting anyio<4,>=3.0.0\n",
            "  Downloading anyio-3.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.9 MB/s \n",
            "\u001b[?25hCollecting sniffio>=1.1\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->farm==0.4.3->farm-haystack==0.2.1) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->farm==0.4.3->farm-haystack==0.2.1) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->farm==0.4.3->farm-haystack==0.2.1) (2.0.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from flask-restplus->farm==0.4.3->farm-haystack==0.2.1) (4.3.3)\n",
            "Collecting aniso8601>=0.82\n",
            "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from flask-restplus->farm==0.4.3->farm-haystack==0.2.1) (2022.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->alembic->mlflow==1.0.0->farm==0.4.3->farm-haystack==0.2.1) (3.8.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->flask-restplus->farm==0.4.3->farm-haystack==0.2.1) (0.18.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->flask-restplus->farm==0.4.3->farm-haystack==0.2.1) (21.4.0)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime->farm==0.4.3->farm-haystack==0.2.1) (2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.7.0->farm==0.4.3->farm-haystack==0.2.1) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval->farm==0.4.3->farm-haystack==0.2.1) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->farm==0.4.3->farm-haystack==0.2.1) (3.1.0)\n",
            "Collecting asgiref>=3.4.0\n",
            "  Downloading asgiref-3.5.0-py3-none-any.whl (22 kB)\n",
            "Collecting h11>=0.8\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: farm-haystack, farm, databricks-cli, langdetect, seqeval\n",
            "  Building wheel for farm-haystack (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for farm-haystack: filename=farm_haystack-0.2.1-py3-none-any.whl size=49321 sha256=81d5119cb7f5213c09a5e3c5d869fb7d94c6e0bdbec3f7b37fb634af7b358b29\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/6f/4a/9f688a2a7b45c6432d59c1006982b884d7e5d107023c3948df\n",
            "  Building wheel for farm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for farm: filename=farm-0.4.3-py3-none-any.whl size=170538 sha256=b222037a04951e77afe21488417cb372498f0b65cedd5e4e4c5e51a80d4783a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/07/97/d9abf00eb85f837e3af37a019f88e22bf070b87edcf2d92c20\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.16.6-py3-none-any.whl size=112631 sha256=1c4295d8f4ee2a6ffd0367af4c20e73f46a7868198ed6c3826e4dfab9b9b27dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/c1/f8/d75a22e789ab6a4dff11f18338c3af4360189aa371295cc934\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=dfdf442455c3a77655c2e9e8176478a5f818a9d16f73bce56ac15edfe9d855ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=754cd8fde1865688f364945343fefcef25bbdbc1d0c56f7fdb21de1e426e295e\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built farm-haystack farm databricks-cli langdetect seqeval\n",
            "Installing collected packages: urllib3, jmespath, smmap, botocore, Werkzeug, websocket-client, sniffio, s3transfer, pyjwt, Mako, gitdb, tokenizers, simplejson, sentencepiece, sacremoses, querystring-parser, gunicorn, gitpython, docker, databricks-cli, boto3, anyio, aniso8601, alembic, transformers, torch, starlette, seqeval, pydantic, onnxruntime, mlflow, h11, flask-restplus, flask-cors, dotmap, asgiref, uvicorn, PyMuPDF, psycopg2-binary, langdetect, fastapi, farm, elasticsearch, elastic-apm, farm-haystack\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: Werkzeug\n",
            "    Found existing installation: Werkzeug 1.0.1\n",
            "    Uninstalling Werkzeug-1.0.1:\n",
            "      Successfully uninstalled Werkzeug-1.0.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.4.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed Mako-1.2.0 PyMuPDF-1.19.6 Werkzeug-0.16.1 alembic-1.7.7 aniso8601-9.0.1 anyio-3.5.0 asgiref-3.5.0 boto3-1.22.3 botocore-1.25.3 databricks-cli-0.16.6 docker-5.0.3 dotmap-1.3.0 elastic-apm-6.9.1 elasticsearch-7.17.3 farm-0.4.3 farm-haystack-0.2.1 fastapi-0.75.2 flask-cors-3.0.10 flask-restplus-0.13.0 gitdb-4.0.9 gitpython-3.1.27 gunicorn-20.1.0 h11-0.13.0 jmespath-1.0.0 langdetect-1.0.9 mlflow-1.0.0 onnxruntime-1.11.1 psycopg2-binary-2.9.3 pydantic-1.9.0 pyjwt-2.3.0 querystring-parser-1.2.4 s3transfer-0.5.2 sacremoses-0.0.49 sentencepiece-0.1.96 seqeval-1.2.2 simplejson-3.17.6 smmap-5.0.0 sniffio-1.2.0 starlette-0.17.1 tokenizers-0.5.2 torch-1.4.0 transformers-2.7.0 urllib3-1.25.11 uvicorn-0.17.6 websocket-client-1.3.2\n"
          ]
        }
      ],
      "source": [
        "! pip install git+https://github.com/deepset-ai/haystack.git@ef9e4f4467a2e265bad72b048a1a3186e40969b1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack import Finder\n",
        "from haystack.database.elasticsearch import ElasticsearchDocumentStore\n",
        "\n",
        "from haystack.retriever.elasticsearch import EmbeddingRetriever\n",
        "from haystack.utils import print_answers\n",
        "import pandas as pd\n",
        "import requests"
      ],
      "metadata": {
        "id": "Mg_aaZs6xIGd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.6.2-linux-x86_64.tar.gz -q\n",
        "! tar -xzf elasticsearch-7.6.2-linux-x86_64.tar.gz\n",
        "! chown -R daemon:daemon elasticsearch-7.6.2\n",
        "\n",
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "es_server = Popen(['elasticsearch-7.6.2/bin/elasticsearch'],\n",
        "                   stdout=PIPE, stderr=STDOUT,\n",
        "                   preexec_fn=lambda: os.setuid(1)  # as daemon\n",
        "                  )\n",
        "# wait until ES has started\n",
        "! sleep 30"
      ],
      "metadata": {
        "id": "1szbJ0pK0EVm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "requests.get('http://localhost:9200/_cluster/health').json()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVxwL1Ej0NbG",
        "outputId": "5ce37035-fcad-4301-add0-1cb5a134ded6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'active_primary_shards': 1,\n",
              " 'active_shards': 1,\n",
              " 'active_shards_percent_as_number': 50.0,\n",
              " 'cluster_name': 'elasticsearch',\n",
              " 'delayed_unassigned_shards': 0,\n",
              " 'initializing_shards': 0,\n",
              " 'number_of_data_nodes': 1,\n",
              " 'number_of_in_flight_fetch': 0,\n",
              " 'number_of_nodes': 1,\n",
              " 'number_of_pending_tasks': 0,\n",
              " 'relocating_shards': 0,\n",
              " 'status': 'yellow',\n",
              " 'task_max_waiting_in_queue_millis': 0,\n",
              " 'timed_out': False,\n",
              " 'unassigned_shards': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.database.elasticsearch import ElasticsearchDocumentStore\n",
        "document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\",\n",
        "                                            index=\"document\",\n",
        "                                            text_field=\"answer\",\n",
        "                                            embedding_field=\"question_emb\",\n",
        "                                            embedding_dim=768,\n",
        "                                            excluded_meta_data=[\"question_emb\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5jjgGw-2Mwc",
        "outputId": "0a580675-679e-45e0-caf0-9768b82781d9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "04/29/2022 16:07:09 - INFO - elasticsearch -   GET http://localhost:9200/ [status:200 request:0.129s]\n",
            "04/29/2022 16:07:09 - INFO - elasticsearch -   PUT http://localhost:9200/document [status:200 request:0.510s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = EmbeddingRetriever(document_store=document_store, embedding_model=\"deepset/sentence_bert\", gpu=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b9X8dnNFdKB",
        "outputId": "0f21cbb6-6613-417d-880d-222670ca004e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "04/29/2022 16:11:10 - INFO - haystack.retriever.elasticsearch -   Init retriever using embeddings of model deepset/sentence_bert\n",
            "04/29/2022 16:11:10 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
            "04/29/2022 16:11:10 - INFO - farm.infer -   Could not find `deepset/sentence_bert` locally. Try to download from model hub ...\n",
            "04/29/2022 16:11:13 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
            "\t We guess it's an *ENGLISH* model ... \n",
            "\t If not: Init the language model by supplying the 'language' param.\n",
            "04/29/2022 16:11:14 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp = requests.get(\"https://raw.githubusercontent.com/uneconomicalfairy14/samvadhini/master/model/faq.csv\")\n",
        "open('faq.csv', 'wb').write(temp.content)\n",
        "# Get dataframe with columns \"question\", \"answer\" and some custom metadata\n",
        "df = pd.read_csv(\"faq.csv\")\n",
        "# Minimal cleaning\n",
        "df.fillna(value=\"\", inplace=True)\n",
        "df[\"question\"] = df[\"question\"].apply(lambda x: x.strip())\n",
        "print(df.head())\n",
        "\n",
        "# Get embeddings for our questions from the FAQs\n",
        "questions = list(df[\"question\"].values)\n",
        "df[\"question_emb\"] = retriever.create_embedding(texts=questions)\n",
        "\n",
        "# Convert Dataframe to list of dicts and index them in our DocumentStore\n",
        "docs_to_index = df.to_dict(orient=\"records\")\n",
        "document_store.write_documents(docs_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SJi1aoCEWKb",
        "outputId": "c4f2b317-6052-40c4-a5d9-ccc6352c86d1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                          question  \\\n",
            "0                                          What if I need to know my correct CGPA?   \n",
            "1  I have cleared my backlog but the result sheet does not show GPA and CGPA. Why?   \n",
            "2  I have cleared my backlog, but the result on ERP is not updated .It still sh...   \n",
            "3                               What do I do if I have not received my grade card?   \n",
            "4  My name/Father's Name/Mother's name is incorrect on grade card. How do I get...   \n",
            "\n",
            "                                                                            answer  \\\n",
            "0  CGPA is seen on the grade card. The CGPA count can be verified by referring ...   \n",
            "1   GPA and CGPA are not mentioned on backlog grade cards. GPA, CGPA will be hi...   \n",
            "2   Grade card shows first attempt GPA and CGPA only. Hence, even if you have c...   \n",
            "3  Grade cards should be collected from the student section of your Department....   \n",
            "4   Name correction on grade cards/PDC needs to be done in ERP at the office of...   \n",
            "\n",
            "                         category  \n",
            "0                      grade card  \n",
            "1                      grade card  \n",
            "2                      grade card  \n",
            "3                      grade card  \n",
            "4  Corrections on grade card /PDC  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 23/23 [03:12<00:00,  8.35s/ Batches]\n",
            "04/29/2022 16:19:09 - INFO - elasticsearch -   POST http://localhost:9200/_bulk [status:200 request:0.322s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finder = Finder(reader=None, retriever=retriever)\n",
        "prediction = finder.get_answers_via_similar_questions(question=\"father name wrong on grade card\", top_k_retriever=10)\n",
        "print_answers(prediction, details=\"medium\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0_XAZlEF-Po",
        "outputId": "230f6f43-09de-4184-e7d4-6067e607cfe4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:02<00:00,  2.00s/ Batches]\n",
            "/usr/local/lib/python3.7/dist-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: The vector functions of the form function(query, doc['field']) are deprecated, and the form function(query, 'field') should be used instead. For example, cosineSimilarity(query, doc['field']) is replaced by cosineSimilarity(query, 'field').\n",
            "  warnings.warn(message, category=ElasticsearchWarning)\n",
            "04/29/2022 16:21:51 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:1.029s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   {   'answer': ' Name correction on grade cards/PDC needs to be done in ERP '\n",
            "                  'at the office of student section of your Department. Kindly '\n",
            "                  'contact the office of student section for the making the '\n",
            "                  'required corrections in ERP. Moreover, student must check '\n",
            "                  'his/her personal details from time to time in their ERP '\n",
            "                  'login.',\n",
            "        'context': ' Name correction on grade cards/PDC needs to be done in '\n",
            "                   'ERP at the office of student section of your Department. '\n",
            "                   'Kindly contact the office of student section for the '\n",
            "                   'making the required corrections in ERP. Moreover, student '\n",
            "                   'must check his/her personal details from time to time in '\n",
            "                   'their ERP login.',\n",
            "        'score': 0.6814560999999999},\n",
            "    {   'answer': ' Name correction on grade cards/PDC needs to be done in ERP '\n",
            "                  'at the office of student section of your Department. Kindly '\n",
            "                  'contact the office of student section for the making the '\n",
            "                  'required corrections in ERP. Moreover, student must check '\n",
            "                  'his/her personal details from time to time in their ERP '\n",
            "                  'login.',\n",
            "        'context': ' Name correction on grade cards/PDC needs to be done in '\n",
            "                   'ERP at the office of student section of your Department. '\n",
            "                   'Kindly contact the office of student section for the '\n",
            "                   'making the required corrections in ERP. Moreover, student '\n",
            "                   'must check his/her personal details from time to time in '\n",
            "                   'their ERP login.',\n",
            "        'score': 0.6814560999999999},\n",
            "    {   'answer': 'Grade cards should be collected from the student section of '\n",
            "                  'your Department. If your grade card is not available at the '\n",
            "                  'student section then you must fill the student assistance '\n",
            "                  'form '\n",
            "                  '(https://mitwpu.edu.in/imaqes/documents/Student-Assistant-Form.pdf)  '\n",
            "                  'at  Department of Examination and get the remark from the '\n",
            "                  'student section on it and submit to Examination '\n",
            "                  'Department.Corrections on grade card /PDC',\n",
            "        'context': 'Grade cards should be collected from the student section '\n",
            "                   'of your Department. If your grade card is not available at '\n",
            "                   'the student section then you must fill the student '\n",
            "                   'assistance form '\n",
            "                   '(https://mitwpu.edu.in/imaqes/documents/Student-Assistant-Form.pdf)  '\n",
            "                   'at  Department of Examination and get the remark from the '\n",
            "                   'student section on it and submit to Examination '\n",
            "                   'Department.Corrections on grade card /PDC',\n",
            "        'score': 0.44519719999999996},\n",
            "    {   'answer': 'Grade cards should be collected from the student section of '\n",
            "                  'your Department. If your grade card is not available at the '\n",
            "                  'student section then you must fill the student assistance '\n",
            "                  'form '\n",
            "                  '(https://mitwpu.edu.in/imaqes/documents/Student-Assistant-Form.pdf)  '\n",
            "                  'at  Department of Examination and get the remark from the '\n",
            "                  'student section on it and submit to Examination '\n",
            "                  'Department.Corrections on grade card /PDC',\n",
            "        'context': 'Grade cards should be collected from the student section '\n",
            "                   'of your Department. If your grade card is not available at '\n",
            "                   'the student section then you must fill the student '\n",
            "                   'assistance form '\n",
            "                   '(https://mitwpu.edu.in/imaqes/documents/Student-Assistant-Form.pdf)  '\n",
            "                   'at  Department of Examination and get the remark from the '\n",
            "                   'student section on it and submit to Examination '\n",
            "                   'Department.Corrections on grade card /PDC',\n",
            "        'score': 0.44519719999999996},\n",
            "    {   'answer': ' Grade card is issued only once. In case, you have lost '\n",
            "                  'your grade card, you can apply for a duplicate grade card '\n",
            "                  'only.',\n",
            "        'context': ' Grade card is issued only once. In case, you have lost '\n",
            "                   'your grade card, you can apply for a duplicate grade card '\n",
            "                   'only.',\n",
            "        'score': 0.43332799999999994},\n",
            "    {   'answer': ' Grade card is issued only once. In case, you have lost '\n",
            "                  'your grade card, you can apply for a duplicate grade card '\n",
            "                  'only.',\n",
            "        'context': ' Grade card is issued only once. In case, you have lost '\n",
            "                   'your grade card, you can apply for a duplicate grade card '\n",
            "                   'only.',\n",
            "        'score': 0.43332799999999994},\n",
            "    {   'answer': 'Consolidated grade card shows revised result of only one '\n",
            "                  'trimester with revised GPA where as a transcript is a '\n",
            "                  'document that includes updated results of all trimesters '\n",
            "                  'with revised GPA and CGPA trimester wise.',\n",
            "        'context': 'Consolidated grade card shows revised result of only one '\n",
            "                   'trimester with revised GPA where as a transcript is a '\n",
            "                   'document that includes updated results of all trimesters '\n",
            "                   'with revised GPA and CGPA trimester wise.',\n",
            "        'score': 0.38859559999999993},\n",
            "    {   'answer': 'Consolidated grade card shows revised result of only one '\n",
            "                  'trimester with revised GPA where as a transcript is a '\n",
            "                  'document that includes updated results of all trimesters '\n",
            "                  'with revised GPA and CGPA trimester wise.',\n",
            "        'context': 'Consolidated grade card shows revised result of only one '\n",
            "                   'trimester with revised GPA where as a transcript is a '\n",
            "                   'document that includes updated results of all trimesters '\n",
            "                   'with revised GPA and CGPA trimester wise.',\n",
            "        'score': 0.38859559999999993},\n",
            "    {   'answer': 'Fee of Rs.500/- + Rs. 100/- per trimester grade card.',\n",
            "        'context': 'Fee of Rs.500/- + Rs. 100/- per trimester grade card.',\n",
            "        'score': 0.3558542},\n",
            "    {   'answer': 'Fee of Rs.500/- + Rs. 100/- per trimester grade card.',\n",
            "        'context': 'Fee of Rs.500/- + Rs. 100/- per trimester grade card.',\n",
            "        'score': 0.3558542}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.pipelines import FAQPipeline\n",
        "\n",
        "pipe = FAQPipeline(retriever=retriever)\n",
        "from haystack.utils import print_answers\n",
        "\n",
        "prediction = pipe.run(query=\"How is the virus spreading?\", params={\"Retriever\": {\"top_k\": 10}})\n",
        "print_answers(prediction, details=\"medium\")"
      ],
      "metadata": {
        "id": "Eai3MNvZF_Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack import Pipeline\n",
        "from haystack.utils import launch_es\n",
        "from haystack.document_stores import ElasticsearchDocumentStore\n",
        "from haystack.nodes import BM25Retriever, EmbeddingRetriever, FARMReader\n",
        "\n",
        "\n",
        "# Initialize DocumentStore and index documents\n",
        "launch_es()\n",
        "document_store = ElasticsearchDocumentStore()\n",
        "document_store.delete_documents()\n",
        "document_store.write_documents(got_docs)\n",
        "\n",
        "# Initialize Sparse retriever\n",
        "es_retriever = BM25Retriever(document_store=document_store)\n",
        "\n",
        "# Initialize dense retriever\n",
        "embedding_retriever = EmbeddingRetriever(\n",
        "    document_store,\n",
        "    model_format=\"sentence_transformers\",\n",
        "    embedding_model=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\",\n",
        ")\n",
        "document_store.update_embeddings(embedding_retriever, update_existing_embeddings=False)\n",
        "\n",
        "# Initialize reader\n",
        "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\")"
      ],
      "metadata": {
        "id": "PTmsYPctJFYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.pipelines import ExtractiveQAPipeline\n",
        "\n",
        "# Prebuilt pipeline\n",
        "p_extractive_premade = ExtractiveQAPipeline(reader=reader, retriever=es_retriever)\n",
        "res = p_extractive_premade.run(\n",
        "    query=\"Where can i get a duplicate grade card\", params={\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}}\n",
        ")\n",
        "print_answers(res, details=\"minimum\")"
      ],
      "metadata": {
        "id": "_ZHmiI6KJIDy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}